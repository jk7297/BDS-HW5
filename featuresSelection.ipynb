{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0101dfb-a839-4590-b276-75ecaa4bea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking of Features by Entropy:\n",
      "1. Survival Months, Entropy: 4.4063\n",
      "2. Tumor Size, Entropy: 3.8617\n",
      "3. Age, Entropy: 3.5246\n",
      "4. Regional Node Examined, Entropy: 3.4138\n",
      "5. Reginol Node Positive, Entropy: 2.1910\n",
      "\n",
      "Ranking of Features by Information Gain:\n",
      "1. Survival Months, Score: 0.1068\n",
      "2. Reginol Node Positive, Score: 0.0045\n",
      "3. Regional Node Examined, Score: 0.0000\n",
      "4. Tumor Size, Score: 0.0000\n",
      "5. Age, Score: 0.0000\n",
      "\n",
      "Ranking of Features Selected by SFS based on Information Gain Scores:\n",
      "1. Survival Months, Score: 0.1068\n",
      "2. Reginol Node Positive, Score: 0.0045\n",
      "3. Regional Node Examined, Score: 0.0000\n",
      "4. Age, Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import entropy\n",
    "from sklearn.feature_selection import mutual_info_classif, SequentialFeatureSelector\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Breast_Cancer_dataset.csv')\n",
    "\n",
    "# Separate features and target variable; 'Status' column is our target value\n",
    "X = data.drop('Status', axis=1)\n",
    "y = data['Status']\n",
    "\n",
    "# 1. Handling Missing Values\n",
    "# Impute numerical data with the median and categorical data with the most frequent value\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "impute_num = SimpleImputer(strategy='median')\n",
    "impute_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X[numerical_columns] = impute_num.fit_transform(X[numerical_columns])\n",
    "X[categorical_columns] = impute_cat.fit_transform(X[categorical_columns])\n",
    "\n",
    "# Get the sorted indices and corresponding feature names for entropies\n",
    "entropy_ranking_indices = np.argsort(entropies)[::-1]  # Sort in descending order of entropy\n",
    "entropy_ranking_features = numerical_columns[entropy_ranking_indices]\n",
    "entropy_ranking_values = entropies[entropy_ranking_indices]\n",
    "\n",
    "# Print ranked features by entropy\n",
    "print(\"Ranking of Features by Entropy:\")\n",
    "for rank, (feature, value) in enumerate(zip(entropy_ranking_features, entropy_ranking_values), start=1):\n",
    "    print(f\"{rank}. {feature}, Entropy: {value:.4f}\")\n",
    "\n",
    "# 2. Detecting and Handling Outliers only for numerical columns\n",
    "# Calculate Q1, Q3 and IQR for each feature\n",
    "Q1 = np.percentile(X[numerical_columns], 25, axis=0)\n",
    "Q3 = np.percentile(X[numerical_columns], 75, axis=0)\n",
    "IQR = Q3 - Q1\n",
    "outlier_step = 1.5 * IQR\n",
    "\n",
    "# Find outliers\n",
    "outliers = ((X[numerical_columns] < (Q1 - outlier_step)) | (X[numerical_columns] > (Q3 + outlier_step))).any(axis=1)\n",
    "X_no_outliers = X[~outliers]\n",
    "y_no_outliers = y[~outliers]\n",
    "\n",
    "# Standardization of Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_no_outliers[numerical_columns])\n",
    "\n",
    "# Add Information gain\n",
    "info_gains = mutual_info_classif(X_scaled, y_no_outliers)\n",
    "\n",
    "# Get the sorted indices and corresponding feature names\n",
    "info_gain_ranking_indices = np.argsort(info_gains)[::-1]  # sort in descending order\n",
    "info_gain_ranking_features = numerical_columns[info_gain_ranking_indices]\n",
    "info_gain_ranking_scores = info_gains[info_gain_ranking_indices]\n",
    "\n",
    "# Print ranked features by IG\n",
    "print()\n",
    "print(\"Ranking of Features by Information Gain:\")\n",
    "for rank, (feature, score) in enumerate(zip(info_gain_ranking_features, info_gain_ranking_scores), start=1):\n",
    "    print(f\"{rank}. {feature}, Score: {score:.4f}\")\n",
    "\n",
    "# Determine the number of features to keep based on Information Gain\n",
    "n_features_total = X_scaled.shape[1]  \n",
    "n_features_to_select = min(5, n_features_total - 1)  \n",
    "\n",
    "# Add SFS with the corrected number of features to select\n",
    "sfs = SequentialFeatureSelector(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    n_features_to_select = n_features_to_select, \n",
    "    direction='forward',\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the SFS to your data\n",
    "sfs.fit(X_scaled, y_no_outliers)\n",
    "\n",
    "# Retrieve the indices of the selected features\n",
    "selected_features_indices = sfs.get_support(indices=True)\n",
    "selected_features = numerical_columns[selected_features_indices]\n",
    "\n",
    "# For features selected by SFS, extract their respective scores from info_gains\n",
    "selected_features_info_gains = info_gains[selected_features_indices]\n",
    "selected_features_info_gains_sorted_indices = np.argsort(selected_features_info_gains)[::-1]\n",
    "\n",
    "# Print ranked features by SFS (based on their IG scores)\n",
    "print()\n",
    "print(\"Ranking of Features Selected by SFS based on Information Gain Scores:\")\n",
    "for rank, idx in enumerate(selected_features_info_gains_sorted_indices, start=1):\n",
    "    feature = numerical_columns[selected_features_indices][idx]\n",
    "    score = selected_features_info_gains[idx]\n",
    "    print(f\"{rank}. {feature}, Score: {score:.4f}\")\n",
    "\n",
    "# Select the top features by entropy from the dataset\n",
    "X_entropy_selected = X_no_outliers[top_features_entropy]\n",
    "\n",
    "# Split the dataset into training and test sets using entropy-selected features\n",
    "X_train_entropy, X_test_entropy, y_train_entropy, y_test_entropy = train_test_split(\n",
    "    X_entropy_selected, y_no_outliers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardization of entropy-selected features\n",
    "scaler_entropy = StandardScaler()\n",
    "X_train_entropy_scaled = scaler_entropy.fit_transform(X_train_entropy)\n",
    "X_test_entropy_scaled = scaler_entropy.transform(X_test_entropy)\n",
    "\n",
    "# Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_no_outliers, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset index for y\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5b4dd81-000b-4d54-9544-4b59af76f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with PCA: 0.90\n",
      "KNN Accuracy with IG selected features: 0.90\n",
      "KNN Accuracy with SFS selected features: 0.88\n",
      "KNN Accuracy with Entropy selected features: 0.90\n"
     ]
    }
   ],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_labels = [self._predict(x) for x in X]\n",
    "        return np.array(predicted_labels)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "# Create and train the KNN classifier\n",
    "knn = KNNClassifier(k=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'KNN Accuracy with PCA: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# Train & predict labels with the KNN model on the features selected by IG\n",
    "knn.fit(X_train[:, top_features_ig_indices], y_train)\n",
    "y_pred_ig = knn.predict(X_test[:, top_features_ig_indices])\n",
    "accuracy_ig = np.mean(y_pred_ig == y_test)\n",
    "print(f'KNN Accuracy with IG selected features: {accuracy_ig:.2f}')\n",
    "\n",
    "# Train & predict labels with the KNN model on the features selected by SFS\n",
    "knn.fit(X_train[:, selected_features_indices], y_train)\n",
    "y_pred_sfs = knn.predict(X_test[:, selected_features_indices])\n",
    "accuracy_sfs = np.mean(y_pred_sfs == y_test)\n",
    "print(f'KNN Accuracy with SFS selected features: {accuracy_sfs:.2f}')\n",
    "\n",
    "# Train & predict labels with the KNN model on the features selected by Entropy\n",
    "knn.fit(X_train[:, entropy_indices], y_train)\n",
    "y_pred_entropy = knn.predict(X_test[:, entropy_indices])\n",
    "accuracy_entropy = np.mean(y_pred_entropy == y_test)\n",
    "print(f'KNN Accuracy with Entropy selected features: {accuracy_entropy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2aab0017-161a-4043-a1b2-388eb49066d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes PCA Accuracy: 0.91\n",
      "Naive Bayes Accuracy with IG selected features: 0.91\n",
      "Naive Bayes Accuracy with SFS selected features: 0.89\n",
      "Naive Bayes Accuracy with Entropy selected features: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Create a Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "print(f'Naive Bayes PCA Accuracy: {nb_accuracy:.2f}')\n",
    "\n",
    "# Naive Bayes with IG selected features\n",
    "nb_classifier_ig = GaussianNB()\n",
    "nb_classifier_ig.fit(X_train[:, top_features_ig_indices], y_train)\n",
    "y_pred_nb_ig = nb_classifier_ig.predict(X_test[:, top_features_ig_indices])\n",
    "nb_accuracy_ig = accuracy_score(y_test, y_pred_nb_ig)\n",
    "print(f'Naive Bayes Accuracy with IG selected features: {nb_accuracy_ig:.2f}')\n",
    "\n",
    "# Naive Bayes with SFS selected features\n",
    "nb_classifier_sfs = GaussianNB()\n",
    "nb_classifier_sfs.fit(X_train[:, selected_features_indices], y_train)\n",
    "y_pred_nb_sfs = nb_classifier_sfs.predict(X_test[:, selected_features_indices])\n",
    "nb_accuracy_sfs = accuracy_score(y_test, y_pred_nb_sfs)\n",
    "print(f'Naive Bayes Accuracy with SFS selected features: {nb_accuracy_sfs:.2f}')\n",
    "\n",
    "# Features selected by SFS\n",
    "selected_info_gains = info_gains[selected_features_indices]\n",
    "selected_features_info_gain_indices = np.argsort(selected_info_gains)[::-1]\n",
    "selected_features_sorted = numerical_columns[selected_features_indices][selected_features_info_gain_indices]\n",
    "selected_info_gains_sorted = selected_info_gains[selected_features_info_gain_indices]\n",
    "\n",
    "# Features selected by Entropy\n",
    "nb_classifier.fit(X_train[:, entropy_indices], y_train)\n",
    "y_pred_nb_entropy = nb_classifier.predict(X_test[:, entropy_indices])\n",
    "nb_accuracy_entropy = accuracy_score(y_test, y_pred_nb_entropy)\n",
    "print(f'Naive Bayes Accuracy with Entropy selected features: {nb_accuracy_entropy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5d20768-75ad-4120-81ec-3cdee3684e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree PCA Accuracy: 0.86\n",
      "Decision Tree Accuracy with IG selected features: 0.85\n",
      "Decision Tree Accuracy with SFS selected features: 0.79\n",
      "Decision Tree Accuracy with Entropy selected features: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(f'Decision Tree PCA Accuracy: {dt_accuracy:.2f}')\n",
    "\n",
    "# Decision Tree with IG selected features\n",
    "dt_classifier_ig = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier_ig.fit(X_train[:, top_features_ig_indices], y_train)\n",
    "y_pred_dt_ig = dt_classifier_ig.predict(X_test[:, top_features_ig_indices])\n",
    "dt_accuracy_ig = accuracy_score(y_test, y_pred_dt_ig)\n",
    "print(f'Decision Tree Accuracy with IG selected features: {dt_accuracy_ig:.2f}')\n",
    "\n",
    "# Decision Tree with SFS selected features\n",
    "dt_classifier_sfs = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier_sfs.fit(X_train[:, selected_features_indices], y_train)\n",
    "y_pred_dt_sfs = dt_classifier_sfs.predict(X_test[:, selected_features_indices])\n",
    "dt_accuracy_sfs = accuracy_score(y_test, y_pred_dt_sfs)\n",
    "print(f'Decision Tree Accuracy with SFS selected features: {dt_accuracy_sfs:.2f}')\n",
    "\n",
    "# Train & predict labels on the training data using entropy-selected features\n",
    "dt_classifier.fit(X_train_entropy_scaled, y_train_entropy)\n",
    "y_pred_dt_entropy = dt_classifier.predict(X_test_entropy_scaled)\n",
    "dt_accuracy_entropy = accuracy_score(y_test_entropy, y_pred_dt_entropy)\n",
    "print(f'Decision Tree Accuracy with Entropy selected features: {dt_accuracy_entropy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2768e42-ac2f-4e44-8c53-490dedec30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest PCA Accuracy: 0.91\n",
      "Random Forest Accuracy with SFS selected features: 0.88\n",
      "Random Forest Accuracy with IG selected features: 0.91\n",
      "Random Forest Accuracy with Entropy selected features: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest PCA Accuracy: {rf_accuracy:.2f}')\n",
    "\n",
    "# Train the Random Forest model on the features selected by SFS\n",
    "rf_classifier_sfs = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier_sfs.fit(X_train[:, selected_features_indices], y_train)\n",
    "y_pred_rf_sfs = rf_classifier_sfs.predict(X_test[:, selected_features_indices])\n",
    "rf_accuracy_sfs = accuracy_score(y_test, y_pred_rf_sfs)\n",
    "print(f'Random Forest Accuracy with SFS selected features: {rf_accuracy_sfs:.2f}')\n",
    "\n",
    "# Train the Random Forest model on the features selected by IG\n",
    "rf_classifier_ig = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier_ig.fit(X_train[:, top_features_ig_indices], y_train)\n",
    "y_pred_rf_ig = rf_classifier_ig.predict(X_test[:, top_features_ig_indices])\n",
    "rf_accuracy_ig = accuracy_score(y_test, y_pred_rf_ig)\n",
    "print(f'Random Forest Accuracy with IG selected features: {rf_accuracy_ig:.2f}')\n",
    "\n",
    "# Train the Random Forest model on the features selected by Entropy\n",
    "rf_classifier.fit(X_train_entropy_scaled, y_train_entropy)\n",
    "y_pred_rf_entropy = rf_classifier.predict(X_test_entropy_scaled)\n",
    "rf_accuracy_entropy = accuracy_score(y_test_entropy, y_pred_rf_entropy)\n",
    "print(f'Random Forest Accuracy with Entropy selected features: {rf_accuracy_entropy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40f2a51f-8858-42ef-9a1c-c2a7ad5bad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting PCA Accuracy: 0.91\n",
      "Gradient Boosting Accuracy with IG selected features: 0.91\n",
      "Gradient Boosting Accuracy with SFS selected features: 0.88\n",
      "Gradient Boosting Accuracy with Entropy selected features: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Create a Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_gb = gb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "print(f'Gradient Boosting PCA Accuracy: {gb_accuracy:.2f}')\n",
    "\n",
    "# Gradient Boosting with IG selected features\n",
    "gb_classifier_ig = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_classifier_ig.fit(X_train[:, top_features_ig_indices], y_train)\n",
    "y_pred_gb_ig = gb_classifier_ig.predict(X_test[:, top_features_ig_indices])\n",
    "gb_accuracy_ig = accuracy_score(y_test, y_pred_gb_ig)\n",
    "print(f'Gradient Boosting Accuracy with IG selected features: {gb_accuracy_ig:.2f}')\n",
    "\n",
    "# Gradient Boosting with SFS selected features\n",
    "gb_classifier_sfs = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_classifier_sfs.fit(X_train[:, selected_features_indices], y_train)\n",
    "y_pred_gb_sfs = gb_classifier_sfs.predict(X_test[:, selected_features_indices])\n",
    "gb_accuracy_sfs = accuracy_score(y_test, y_pred_gb_sfs)\n",
    "print(f'Gradient Boosting Accuracy with SFS selected features: {gb_accuracy_sfs:.2f}')\n",
    "\n",
    "# Gradient Boosting with Entropy selected features\n",
    "gb_classifier_entropy = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_classifier_entropy.fit(X_train_entropy_scaled, y_train_entropy)\n",
    "y_pred_gb_entropy = gb_classifier_entropy.predict(X_test_entropy_scaled)\n",
    "gb_accuracy_entropy = accuracy_score(y_test_entropy, y_pred_gb_entropy)\n",
    "print(f'Gradient Boosting Accuracy with Entropy selected features: {gb_accuracy_entropy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52c03b4b-192b-47bb-9a5e-86ab6967802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network PCA Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy with IG: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy with SFS: 0.89\n",
      "Neural Network Accuracy with Entropy: 0.92\n"
     ]
    }
   ],
   "source": [
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=400, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_nn = nn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "nn_accuracy = accuracy_score(y_test, y_pred_nn)\n",
    "print(f'Neural Network PCA Accuracy: {nn_accuracy:.2f}')\n",
    "\n",
    "# Neural Network with IG selected features\n",
    "nn_classifier_ig = MLPClassifier(hidden_layer_sizes=(100,), max_iter=400, activation='relu', solver='adam', random_state=42)\n",
    "nn_classifier_ig.fit(X_train[:, top_features_ig_indices], y_train)\n",
    "y_pred_nn_ig = nn_classifier_ig.predict(X_test[:, top_features_ig_indices])\n",
    "nn_accuracy_ig = accuracy_score(y_test, y_pred_nn_ig)\n",
    "print(f'Neural Network Accuracy with IG: {nn_accuracy_ig:.2f}')\n",
    "\n",
    "# Neural Network with SFS selected features\n",
    "nn_classifier_sfs = MLPClassifier(hidden_layer_sizes=(100,), max_iter=400, activation='relu', solver='adam', random_state=42)\n",
    "nn_classifier_sfs.fit(X_train[:, selected_features_indices], y_train)\n",
    "y_pred_nn_sfs = nn_classifier_sfs.predict(X_test[:, selected_features_indices])\n",
    "nn_accuracy_sfs = accuracy_score(y_test, y_pred_nn_sfs)\n",
    "print(f'Neural Network Accuracy with SFS: {nn_accuracy_sfs:.2f}')\n",
    "\n",
    "# Neural Network with Entropy selected features\n",
    "nn_classifier_entropy = MLPClassifier(hidden_layer_sizes=(100,), max_iter=400, activation='relu', solver='adam', random_state=42)\n",
    "nn_classifier_entropy.fit(X_train_entropy_scaled, y_train_entropy)\n",
    "y_pred_nn_entropy = nn_classifier_entropy.predict(X_test_entropy_scaled)\n",
    "nn_accuracy_entropy = accuracy_score(y_test_entropy, y_pred_nn_entropy)\n",
    "print(f'Neural Network Accuracy with Entropy: {nn_accuracy_entropy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90be87a8-b7a5-42c1-b6c0-3b8e85b39137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
      "Best score for Random Forest: 0.8976406533575318\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Alive       0.92      0.98      0.95       609\n",
      "        Dead       0.70      0.38      0.49        80\n",
      "\n",
      "    accuracy                           0.91       689\n",
      "   macro avg       0.81      0.68      0.72       689\n",
      "weighted avg       0.90      0.91      0.90       689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier object and GridSearchCV object\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\n",
    "print(\"Best score for Random Forest:\", rf_grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "rf_best = rf_grid_search.best_estimator_\n",
    "rf_predictions = rf_best.predict(X_test)\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b2da709-7980-4418-99ab-8fb50d48e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Neural Network: {'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "Best score for Neural Network: 0.8980036297640653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Alive       0.93      0.99      0.96       609\n",
      "        Dead       0.80      0.41      0.55        80\n",
      "\n",
      "    accuracy                           0.92       689\n",
      "   macro avg       0.87      0.70      0.75       689\n",
      "weighted avg       0.91      0.92      0.91       689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for MLPClassifier\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Create an MLPClassifier object and GridSearchCV object\n",
    "nn_model = MLPClassifier(max_iter=400, random_state=42)\n",
    "nn_grid_search = GridSearchCV(estimator=nn_model, param_grid=nn_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "nn_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters for Neural Network:\", nn_grid_search.best_params_)\n",
    "print(\"Best score for Neural Network:\", nn_grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "nn_best = nn_grid_search.best_estimator_\n",
    "nn_predictions = nn_best.predict(X_test)\n",
    "print(classification_report(y_test, nn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a36cb-0b6c-4a14-aafb-a4a01436754e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
